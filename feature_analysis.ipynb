{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64876b6a",
   "metadata": {},
   "source": [
    "# Feature Analysis\n",
    "\n",
    "This notebook will visually display some dataframes to understand more context behind the datasets I'm using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f666a43",
   "metadata": {},
   "source": [
    "### 1. Post GPT prediction dataset\n",
    "\n",
    "The following files [results/explain_features.csv](results/explain_features.csv) and [results/predict_then_explain_results.csv](results/predict_then_explain_results.csv) contains GPT's prediction of which of two responses likely changed the OP's opinion. Each row consists of the following:\n",
    "\n",
    "- response_1, response_2 : The two responses to the OP. The prediction one is randomized\n",
    "- prediction_response : either 1 or 2 depending on the response\n",
    "- prediction : either 1 or 2 depending on GPT's guess at the prediction_response\n",
    "- explanation : GPT's reasoning as to why they chose a particular response\n",
    "- temperature : (set to 0) how much variance there is in GPT's responses are\n",
    "\n",
    "The following is what the first few lines of the dataset look like (for just explain-then-predict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "665ad132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_1</th>\n",
       "      <th>response_2</th>\n",
       "      <th>correct_response</th>\n",
       "      <th>prediction</th>\n",
       "      <th>correct</th>\n",
       "      <th>explanation</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"The Kurds\" are an ethnic group, with a simila...</td>\n",
       "      <td>&amp;gt;1) The Kurds espouse a number of western v...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Response 1 provides a more detailed and nuance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actually 2040 isn't a bad estimate. [In 2012 C...</td>\n",
       "      <td>Have you looked into googles plans for self dr...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Response 2 provides concrete examples and evid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;gt; This is why I think that giving the reaso...</td>\n",
       "      <td>In principle, I agree with you that parents do...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Response 2 presents a strong argument by highl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If we can make them then it's highly probable ...</td>\n",
       "      <td>You've already addressed a bunch of points her...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Response 2 is more persuasive because it chall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If someone uses the \"backdoor\" to my bathroom ...</td>\n",
       "      <td>The arguments against surveillance are nearly ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Response 1 is more persuasive because it direc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          response_1  \\\n",
       "0  \"The Kurds\" are an ethnic group, with a simila...   \n",
       "1  Actually 2040 isn't a bad estimate. [In 2012 C...   \n",
       "2  &gt; This is why I think that giving the reaso...   \n",
       "3  If we can make them then it's highly probable ...   \n",
       "4  If someone uses the \"backdoor\" to my bathroom ...   \n",
       "\n",
       "                                          response_2  correct_response  \\\n",
       "0  &gt;1) The Kurds espouse a number of western v...                 1   \n",
       "1  Have you looked into googles plans for self dr...                 2   \n",
       "2  In principle, I agree with you that parents do...                 2   \n",
       "3  You've already addressed a bunch of points her...                 2   \n",
       "4  The arguments against surveillance are nearly ...                 1   \n",
       "\n",
       "   prediction  correct                                        explanation  \\\n",
       "0           1     True  Response 1 provides a more detailed and nuance...   \n",
       "1           2     True  Response 2 provides concrete examples and evid...   \n",
       "2           2     True  Response 2 presents a strong argument by highl...   \n",
       "3           2     True  Response 2 is more persuasive because it chall...   \n",
       "4           1     True  Response 1 is more persuasive because it direc...   \n",
       "\n",
       "   temperature  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"results/explain_then_predict_results.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20ecc1",
   "metadata": {},
   "source": [
    "Now, let's pick a random one so you can see the full responses and the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f026bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE 1: What would you suggest Christians do? Not vote? Move out of the US?\n",
      "To where? My family is christian. We pay taxes, work, support the economy, are\n",
      "contributing members of society. By some definitions, thats \"supporting the\n",
      "actions of the US\". I am curious what you think the alternative is?\n",
      "\n",
      "RESPONSE 2: You're treating the United States as some unified identity whose\n",
      "actions one could support or oppose.  You're adding up government policies and\n",
      "cultural trends and treating them as some sort of package.  You're also adding\n",
      "together different actions by different levels of government and in different\n",
      "places.  Nobody supports everything that goes on in the United States.  It would\n",
      "be a contradiction to do so, as different things happen in different places, or\n",
      "at different levels of government.\n",
      "\n",
      "EXPLANATION: Response 1 provides a more logical and structured argument by\n",
      "breaking down the idea of supporting the United States into different\n",
      "components. It addresses the complexity of the situation by highlighting the\n",
      "diversity of actions and policies within the country, making it difficult to\n",
      "categorize support or opposition in a binary manner. Response 2, on the other\n",
      "hand, focuses more on personal anecdotes and questions rather than directly\n",
      "addressing the OP's view.\n",
      "\n",
      "CORRECT RESPONSE: 2\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "row = df.sample(1).iloc[0]\n",
    "\n",
    "print(textwrap.fill(\"RESPONSE 1: \" + row['response_1'], width=80))\n",
    "print(\"\")\n",
    "print(textwrap.fill(\"RESPONSE 2: \" + row['response_2'], width=80))\n",
    "print(\"\")\n",
    "print(textwrap.fill(\"EXPLANATION: \" + row['explanation'], width=80))\n",
    "print(\"\")\n",
    "print(\"CORRECT RESPONSE:\", row[\"correct_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0bdaf",
   "metadata": {},
   "source": [
    "The following are some functions to calculate statistical signifiance for proportion function to help calculate p-values for later. You can skip to the next text box (would recommend minimizing if possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dc7934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "def one_prop_test(p_hat, p0, n, tail='left'):\n",
    "\n",
    "    se = (p0 * (1 - p0) / n)**(1/2)\n",
    "    z = (p_hat - p0) / se\n",
    "\n",
    "    if tail == 'right':\n",
    "        p_val = 1 - norm.cdf(z)\n",
    "    elif tail == 'left':\n",
    "        p_val = norm.cdf(z)\n",
    "    else:\n",
    "        raise ValueError(\"tail must be left or right\")\n",
    "\n",
    "    return p_val, z\n",
    "\n",
    "def one_sample_ttest(column, mu0=0, tail='left'):\n",
    "    n = column.shape[0]\n",
    "    sample_mean = column.mean()\n",
    "    sample_std = column.std(ddof=1)\n",
    "\n",
    "    t = (sample_mean - mu0) / (sample_std / (n ** 0.5))\n",
    "\n",
    "    if tail == 'right':\n",
    "            p_val = 1 - stats.t.cdf(t, df=n - 1)\n",
    "    elif tail == 'left':\n",
    "        p_val = stats.t.cdf(t, df=n - 1)\n",
    "    else:\n",
    "        raise ValueError(\"tail must be left or right\")\n",
    "\n",
    "    return p_val, t\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f818680",
   "metadata": {},
   "source": [
    "We now calculate the p-values for each dataset to determine statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38bd829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain-Then-Predict Accuracy: 290 / 500 = 0.58\n",
      "p-value for Explain-Then-Predict: 0.0002. z-score for Explain-Then-Predict: 3.58\n",
      "\n",
      "Predict-Then-Explain Accuracy: 270 / 500 = 0.54\n",
      "p-value for Explain-Then-Predict: 0.0368. z-score for Explain-Then-Predict: 1.79\n"
     ]
    }
   ],
   "source": [
    "df_ETP = pd.read_csv(\"results/explain_then_predict_results.csv\")\n",
    "ETP_correct = (df_ETP['prediction'] == df_ETP['correct_response']).sum()\n",
    "df_PTE = pd.read_csv(\"results/predict_then_explain_results.csv\")\n",
    "PTE_correct = (df_PTE['prediction'] == df_PTE['correct_response']).sum()\n",
    "print(f\"Explain-Then-Predict Accuracy: {ETP_correct} / 500 = {ETP_correct/500}\")\n",
    "ETP_p, ETP_z = one_prop_test(0.58, 0.5, 500, 'right')\n",
    "print(f\"p-value for Explain-Then-Predict: {round(ETP_p,4)}. z-score for Explain-Then-Predict: {round(ETP_z,2)}\\n\")\n",
    "print(f\"Predict-Then-Explain Accuracy: {PTE_correct} / 500 = {PTE_correct/500}\")\n",
    "PTE_p, PTE_z = one_prop_test(0.54, 0.5, 500, 'right')\n",
    "print(f\"p-value for Explain-Then-Predict: {round(PTE_p,4)}. z-score for Explain-Then-Predict: {round(PTE_z,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e68e8a",
   "metadata": {},
   "source": [
    "So both are statistically significant (assuming threshold is p=0.05), meaning that GPT is guessing at a rate higher than random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680c95b",
   "metadata": {},
   "source": [
    "### 2. Features of user responses dataset \n",
    "\n",
    "These datasets [(results/predict_features.csv](results/predict_features.csv) and [results/explain_features.csv](results/explain_features.csv) contained some features we extracted from each dataset. Namely the following:\n",
    "\n",
    "- **word_count** : The word count of each response\n",
    "- **response_1/2_valence** : The average valence score, or how pleasant a word sounds, of each text, as determined by the [NRC-VAD Lexicon](https://saifmohammad.com/WebPages/nrc-vad.html)\n",
    "- **response_1/2_arousal** : The average arousal score, or how emotionally intense a word is, of each text, as determined by the [NRC-VAD Lexicon](https://saifmohammad.com/WebPages/nrc-vad.html)\n",
    "- **response_1/2_dominance** : The average dominance score, or how much degree of control a word has, of each text, as determined by the [NRC-VAD Lexicon](https://saifmohammad.com/WebPages/nrc-vad.html)\n",
    "- **response_1/2_concreteness** : The average concreteness score, or how perceptible or intangible a word is, of each text, as determined by [https://link.springer.com/article/10.3758/s13428-013-0403-5#Sec10](https://link.springer.com/article/10.3758/s13428-013-0403-5#Sec10)\n",
    "\n",
    "For valence, arousal, dominance and concreteness, words were normalized to a **range of 0 to 1**, and only words with a **value above 0.65** were included, as to not dilute the points. Additionally, the difference of the correct response minus the incorrect response was also tracked for each entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc998d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_1_word_count</th>\n",
       "      <th>response_2_word_count</th>\n",
       "      <th>word_count_prediction_difference</th>\n",
       "      <th>response_1_valence</th>\n",
       "      <th>response_2_valence</th>\n",
       "      <th>valence_prediction_difference</th>\n",
       "      <th>response_1_arousal</th>\n",
       "      <th>response_2_arousal</th>\n",
       "      <th>arousal_prediction_difference</th>\n",
       "      <th>response_1_dominance</th>\n",
       "      <th>response_2_dominance</th>\n",
       "      <th>dominance_prediction_difference</th>\n",
       "      <th>response_1_concreteness</th>\n",
       "      <th>response_2_concreteness</th>\n",
       "      <th>concreteness_prediction_difference</th>\n",
       "      <th>response_1_link_count</th>\n",
       "      <th>response_2_link_count</th>\n",
       "      <th>link_count_prediction_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161</td>\n",
       "      <td>364</td>\n",
       "      <td>-203</td>\n",
       "      <td>0.283250</td>\n",
       "      <td>0.538545</td>\n",
       "      <td>-0.255295</td>\n",
       "      <td>0.940750</td>\n",
       "      <td>0.898875</td>\n",
       "      <td>0.041875</td>\n",
       "      <td>0.955833</td>\n",
       "      <td>0.842042</td>\n",
       "      <td>0.113792</td>\n",
       "      <td>0.818769</td>\n",
       "      <td>0.838077</td>\n",
       "      <td>-0.019308</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148</td>\n",
       "      <td>119</td>\n",
       "      <td>-29</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.212433</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>-0.159267</td>\n",
       "      <td>0.862800</td>\n",
       "      <td>0.823231</td>\n",
       "      <td>-0.039569</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>384</td>\n",
       "      <td>495</td>\n",
       "      <td>111</td>\n",
       "      <td>0.858675</td>\n",
       "      <td>0.720577</td>\n",
       "      <td>-0.138098</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.386167</td>\n",
       "      <td>-0.478333</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>0.890500</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.833913</td>\n",
       "      <td>0.831348</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>80</td>\n",
       "      <td>-23</td>\n",
       "      <td>0.699778</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>0.203022</td>\n",
       "      <td>0.693667</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>-0.575667</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.890500</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>-0.059333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.743250</td>\n",
       "      <td>0.179750</td>\n",
       "      <td>0.884250</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.884889</td>\n",
       "      <td>-0.040889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response_1_word_count  response_2_word_count  \\\n",
       "0                    161                    364   \n",
       "1                    148                    119   \n",
       "2                    384                    495   \n",
       "3                    103                     80   \n",
       "4                    110                    111   \n",
       "\n",
       "   word_count_prediction_difference  response_1_valence  response_2_valence  \\\n",
       "0                              -203            0.283250            0.538545   \n",
       "1                               -29            0.745900            0.958333   \n",
       "2                               111            0.858675            0.720577   \n",
       "3                               -23            0.699778            0.902800   \n",
       "4                                -1            0.744000            0.274000   \n",
       "\n",
       "   valence_prediction_difference  response_1_arousal  response_2_arousal  \\\n",
       "0                      -0.255295            0.940750            0.898875   \n",
       "1                       0.212433            0.922000            0.981500   \n",
       "2                      -0.138098            0.864500            0.386167   \n",
       "3                       0.203022            0.693667            0.118000   \n",
       "4                       0.470000            0.923000            0.743250   \n",
       "\n",
       "   arousal_prediction_difference  response_1_dominance  response_2_dominance  \\\n",
       "0                       0.041875              0.955833              0.842042   \n",
       "1                       0.059500              0.912667              0.753400   \n",
       "2                      -0.478333              0.877100              0.890500   \n",
       "3                      -0.575667              0.707000              0.890500   \n",
       "4                       0.179750              0.884250              0.874500   \n",
       "\n",
       "   dominance_prediction_difference  response_1_concreteness  \\\n",
       "0                         0.113792                 0.818769   \n",
       "1                        -0.159267                 0.862800   \n",
       "2                         0.013400                 0.833913   \n",
       "3                         0.183500                 0.870000   \n",
       "4                         0.009750                 0.844000   \n",
       "\n",
       "   response_2_concreteness  concreteness_prediction_difference  \\\n",
       "0                 0.838077                           -0.019308   \n",
       "1                 0.823231                           -0.039569   \n",
       "2                 0.831348                           -0.002565   \n",
       "3                 0.810667                           -0.059333   \n",
       "4                 0.884889                           -0.040889   \n",
       "\n",
       "   response_1_link_count  response_2_link_count  \\\n",
       "0                      0                      1   \n",
       "1                      3                      0   \n",
       "2                      0                      0   \n",
       "3                      0                      0   \n",
       "4                      0                      0   \n",
       "\n",
       "   link_count_prediction_difference  \n",
       "0                                -1  \n",
       "1                                -3  \n",
       "2                                 0  \n",
       "3                                 0  \n",
       "4                                 0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('results/explain_features.csv')\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dab618",
   "metadata": {},
   "source": [
    "Here we can see the average difference for the correct responses vs the incorrect responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2149bae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count_prediction_difference     -12.072000\n",
      "valence_prediction_difference         -0.000241\n",
      "arousal_prediction_difference          0.017882\n",
      "dominance_prediction_difference       -0.025396\n",
      "concreteness_prediction_difference     0.000396\n",
      "link_count_prediction_difference       0.018000\n",
      "dtype: float64 \n",
      "\n",
      "link_count_prediction_difference for texts that contain a link: 0.0743801652892562\n"
     ]
    }
   ],
   "source": [
    "df_diff = df2[[\"word_count_prediction_difference\", \"valence_prediction_difference\", \"arousal_prediction_difference\", \"dominance_prediction_difference\", \"concreteness_prediction_difference\", \"link_count_prediction_difference\"]]\n",
    "print(df_diff.mean(),\"\\n\")\n",
    "\n",
    "\n",
    "mean_link_diff = df2.loc[\n",
    "    df2[\"link_count_prediction_difference\"] != 0, \n",
    "    \"link_count_prediction_difference\"\n",
    "].mean()\n",
    "print(f\"link_count_prediction_difference for texts that contain a link: {mean_link_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f4171",
   "metadata": {},
   "source": [
    "Let's test for statistical significant using a standard t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99afdf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 t     p_val\n",
      "word_count_prediction_difference         -1.301163  0.096902\n",
      "valence_prediction_difference            -0.018984  0.492431\n",
      "arousal_prediction_difference             0.921249  0.178683\n",
      "dominance_prediction_difference          -1.510873  0.065727\n",
      "concreteness_prediction_difference        0.252301  0.400456\n",
      "link_count_prediction_difference          0.272977  0.392492\n",
      "link_count_prediction_difference_nonzero  0.272182  0.392975\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for col in df_diff.columns:\n",
    "    #Determine tail direction\n",
    "    mean_val = df_diff[col].mean()\n",
    "    tail = 'left' if mean_val < 0 else 'right'\n",
    "\n",
    "    p_val, t = one_sample_ttest(df_diff[col], mu0=0, tail=tail)\n",
    "    results[col] = {\"t\": t, \"p_val\": p_val}\n",
    "\n",
    "#for links without posts with 0 links\n",
    "nonzero_links = df2.loc[\n",
    "    df2[\"link_count_prediction_difference\"] != 0,\n",
    "    \"link_count_prediction_difference\"\n",
    "]\n",
    "\n",
    "mean_link_diff = nonzero_links.mean()\n",
    "tail = 'left' if mean_link_diff < 0 else 'right'\n",
    "\n",
    "p_val, t = one_sample_ttest(nonzero_links, mu0=0, tail=tail)\n",
    "\n",
    "results[\"link_count_prediction_difference_nonzero\"] = {\n",
    "    \"t\": t,\n",
    "    \"p_val\": p_val\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ea7b6",
   "metadata": {},
   "source": [
    "We do the same for [**predict_then_explain**](results/predict_features.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49151068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_count_prediction_difference     -3.276000\n",
      "valence_prediction_difference        -0.008216\n",
      "arousal_prediction_difference         0.004585\n",
      "dominance_prediction_difference      -0.041168\n",
      "concreteness_prediction_difference    0.002549\n",
      "link_count_prediction_difference      0.022000\n",
      "dtype: float64 \n",
      "\n",
      "link_count_prediction_difference for texts that contain a link: 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('results/predict_features.csv')\n",
    "df_diff = df2[[\"word_count_prediction_difference\", \"valence_prediction_difference\", \"arousal_prediction_difference\", \"dominance_prediction_difference\", \"concreteness_prediction_difference\", \"link_count_prediction_difference\"]]\n",
    "print(df_diff.mean(),\"\\n\")\n",
    "\n",
    "\n",
    "mean_link_diff = df2.loc[\n",
    "    df2[\"link_count_prediction_difference\"] != 0, \n",
    "    \"link_count_prediction_difference\"\n",
    "].mean()\n",
    "print(f\"link_count_prediction_difference for texts that contain a link: {mean_link_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac4aa33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 t     p_val\n",
      "word_count_prediction_difference         -0.352545  0.362289\n",
      "valence_prediction_difference            -0.646406  0.259157\n",
      "arousal_prediction_difference             0.236018  0.406758\n",
      "dominance_prediction_difference          -2.458374  0.007148\n",
      "concreteness_prediction_difference        1.629797  0.051888\n",
      "link_count_prediction_difference          0.333650  0.369392\n",
      "link_count_prediction_difference_nonzero  0.332718  0.369964\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for col in df_diff.columns:\n",
    "    #Determine tail direction\n",
    "    mean_val = df_diff[col].mean()\n",
    "    tail = 'left' if mean_val < 0 else 'right'\n",
    "\n",
    "    p_val, t = one_sample_ttest(df_diff[col], mu0=0, tail=tail)\n",
    "    results[col] = {\"t\": t, \"p_val\": p_val}\n",
    "\n",
    "#for links without posts with 0 links\n",
    "nonzero_links = df2.loc[\n",
    "    df2[\"link_count_prediction_difference\"] != 0,\n",
    "    \"link_count_prediction_difference\"\n",
    "]\n",
    "\n",
    "mean_link_diff = nonzero_links.mean()\n",
    "tail = 'left' if mean_link_diff < 0 else 'right'\n",
    "\n",
    "p_val, t = one_sample_ttest(nonzero_links, mu0=0, tail=tail)\n",
    "\n",
    "results[\"link_count_prediction_difference_nonzero\"] = {\n",
    "    \"t\": t,\n",
    "    \"p_val\": p_val\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
